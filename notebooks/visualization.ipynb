{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize test performance with Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset import NTUDataset, get_train_val_set\n",
    "from model import ConvLSTM\n",
    "\n",
    "\n",
    "def get_train_val_loader(params, val_pct=0.2):\n",
    "    train_samples, val_samples = get_train_val_set(data_path=params.data_path, val_pct=val_pct, temporal_aug_k=params.temporal_aug_k)\n",
    "    print(f'Train samples: {len(train_samples)} || Validation samples: {len(val_samples)}')\n",
    "    \n",
    "    # Apply transform to normalize the data\n",
    "    # transform = transforms.Normalize((0.5), (0.5))\n",
    "    \n",
    "    # Load train and validation dataset\n",
    "    train_set = NTUDataset(sample_set=train_samples, params=params, transform=None)\n",
    "    val_set = NTUDataset(sample_set=val_samples, params=params, transform=None)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=params.BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=params.BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def save_model(model):\n",
    "    current_time = datetime.now()\n",
    "    current_time = current_time.strftime(\"%m_%d_%Y_%H_%M\")\n",
    "    torch.save(model.state_dict(), f'../saved_models/ntu_lstm_{current_time}.pth')\n",
    "    \n",
    "    \n",
    "def build_test_stats(preds, actual, acc, params):\n",
    "    print(f'Model accuracy: {acc}')\n",
    "    \n",
    "    # For confusion matrix\n",
    "    preds = [int(k) for k in preds]\n",
    "    actual = [int(k) for k in actual]\n",
    "\n",
    "    cf = confusion_matrix(actual, preds, labels=list(range(params.num_classes)))\n",
    "    return cf\n",
    "\n",
    "\n",
    "def train(model, train_loader, loss_function, optimizer, params):\n",
    "    print('Training...')\n",
    "    for epoch in range(params.n_epochs):\n",
    "        for batch in tqdm(train_loader):\n",
    "            inputs, labels = batch[0].to(device).float(), batch[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch: {epoch} | Loss: {loss}')\n",
    "\n",
    "    return model\n",
    "\n",
    "def test(model, test_loader):\n",
    "    print('Testing...')\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    preds = []\n",
    "    actual = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device).float(), labels.to(device)\n",
    "            class_outputs = model(inputs)\n",
    "            _, class_prediction = torch.max(class_outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (class_prediction == labels).sum().item()\n",
    "            preds.extend(list(class_prediction.to(dtype=torch.int64)))\n",
    "            actual.extend(list(labels.to(dtype=torch.int64)))\n",
    "\n",
    "    acc = 100*correct/total\n",
    "    return preds, actual, acc\n",
    "\n",
    "\n",
    "def main(params):\n",
    "    # Initialize some variables to track progress\n",
    "    accs = []\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = ConvLSTM(params=params).to(device)\n",
    "    \n",
    "    # Use parallel computing if available\n",
    "    if device.type == 'cuda' and n_gpus > 1:\n",
    "        model = nn.DataParallel(model, list(range(n_gpus)))\n",
    "        \n",
    "    # Loss Function and Optimizer (can use weight=class_weights if it is a disbalanced dataset)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Get train and validation loaders\n",
    "    train_loader, val_loader = get_train_val_loader(params, val_pct=0.2)\n",
    "    \n",
    "    # Train the model\n",
    "    model = train(model, train_loader, loss_function, optimizer, params)\n",
    "    save_model(model)\n",
    "\n",
    "    # Get training accuracy\n",
    "    preds, actual, acc = test(model, train_loader)\n",
    "    build_test_stats(preds, actual, acc, params)\n",
    "    \n",
    "    # Validate the model\n",
    "    preds, actual, acc = test(model, val_loader)\n",
    "    build_test_stats(preds, actual, acc, params)\n",
    "    \n",
    "\n",
    "## Optional code to load and test a model\n",
    "def load_test_model(params, model_path):\n",
    "    model = ConvLSTM(params=params).to(device)\n",
    "    # Use this to fix keyError in the model when using DataParallel while training\n",
    "    if device.type == 'cuda' and n_gpus > 1:\n",
    "        model = nn.DataParallel(model, list(range(n_gpus)))\n",
    "    train_loader, val_loader = get_train_val_loader(params, val_pct=0.2)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval() # To set dropout and batchnormalization OFF\n",
    "    preds, actual, acc = test(model, val_loader)\n",
    "    return build_test_stats(preds, actual, acc, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n"
     ]
    }
   ],
   "source": [
    "params = {'mode': 'inference', 'model_path': '/home/youngjoon/DEV/ntu-skeleton/code/saved_models/ntu_lstm_10_30_2022_14_34.pth', 'kp_shape': [18, 3], 'seg_size': 50, \n",
    "          'data_path': '/home/youngjoon/DEV/NTU_VIBE_NPY(Raw)/', \n",
    "          'BATCH_SIZE': 8, 'temporal_aug_k': 3, 'k_fold': 1, 'n_epochs': 20, 'num_classes': 13, 'bcc': 32, 'num_channels': 1,\n",
    "          'num_joints': 18, 'num_coord': 3}\n",
    "params = edict(params)\n",
    "# Check for GPUs\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpus = torch.cuda.device_count()\n",
    "print(f'Number of GPUs available: {n_gpus}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Video Samples: 1191 || Total Participants: 104 || Train Participants: 84 || Validation Participants: 20\n",
      "Train samples: 2952 || Validation samples: 621\n",
      "Evaluating mean and std for the training set...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 3750 into shape (50,1,18,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/DEV/ntu-skeleton/notebooks/dataset.py:85\u001b[0m, in \u001b[0;36mNTUDataset.get_mean_std\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     \u001b[39m# Read pickled file for mean and std\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m     mean \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39;49mload(mean_path))\n\u001b[1;32m     86\u001b[0m     std \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39mload(std_path))\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/numpy/lib/npyio.py:390\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    391\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../mean_std/mean_50_18_3.npy'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cf \u001b[38;5;241m=\u001b[39m \u001b[43mload_test_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df_cm \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(cf, index\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m121\u001b[39m)], columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m121\u001b[39m)])\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m7\u001b[39m))\n",
      "Cell \u001b[0;32mIn [2], line 128\u001b[0m, in \u001b[0;36mload_test_model\u001b[0;34m(params, model_path)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m n_gpus \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    127\u001b[0m     model \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDataParallel(model, \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(n_gpus)))\n\u001b[0;32m--> 128\u001b[0m train_loader, val_loader \u001b[38;5;241m=\u001b[39m \u001b[43mget_train_val_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_pct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(model_path))\n\u001b[1;32m    130\u001b[0m model\u001b[38;5;241m.\u001b[39meval() \u001b[38;5;66;03m# To set dropout and batchnormalization OFF\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [2], line 26\u001b[0m, in \u001b[0;36mget_train_val_loader\u001b[0;34m(params, val_pct)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_samples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m || Validation samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(val_samples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Apply transform to normalize the data\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# transform = transforms.Normalize((0.5), (0.5))\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Load train and validation dataset\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m train_set \u001b[38;5;241m=\u001b[39m \u001b[43mNTUDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m val_set \u001b[38;5;241m=\u001b[39m NTUDataset(sample_set\u001b[38;5;241m=\u001b[39mval_samples, params\u001b[38;5;241m=\u001b[39mparams, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_set, batch_size\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/DEV/ntu-skeleton/notebooks/dataset.py:19\u001b[0m, in \u001b[0;36mNTUDataset.__init__\u001b[0;34m(self, sample_set, params, transform)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_channels \u001b[39m=\u001b[39m params\u001b[39m.\u001b[39mnum_channels\n\u001b[1;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39m=\u001b[39m transform\n\u001b[0;32m---> 19\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_mean_std()\n",
      "File \u001b[0;32m~/DEV/ntu-skeleton/notebooks/dataset.py:89\u001b[0m, in \u001b[0;36mNTUDataset.get_mean_std\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEvaluating mean and std for the training set...\u001b[39m\u001b[39m'\u001b[39m)         \n\u001b[0;32m---> 89\u001b[0m     X \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_sample(sample_name)[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m sample_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_set])\n\u001b[1;32m     90\u001b[0m     X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseg_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_channels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkp_shape[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkp_shape[\u001b[39m1\u001b[39m])\n\u001b[1;32m     91\u001b[0m     mean \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/DEV/ntu-skeleton/notebooks/dataset.py:89\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEvaluating mean and std for the training set...\u001b[39m\u001b[39m'\u001b[39m)         \n\u001b[0;32m---> 89\u001b[0m     X \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_sample(sample_name)[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m sample_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_set])\n\u001b[1;32m     90\u001b[0m     X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseg_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_channels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkp_shape[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkp_shape[\u001b[39m1\u001b[39m])\n\u001b[1;32m     91\u001b[0m     mean \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/DEV/ntu-skeleton/notebooks/dataset.py:54\u001b[0m, in \u001b[0;36mNTUDataset.read_sample\u001b[0;34m(self, sample_name)\u001b[0m\n\u001b[1;32m     51\u001b[0m action_class \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(sample_name\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39mA\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m1\u001b[39m][:\u001b[39m3\u001b[39m]) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[39m# Before returning the sample_kp, change its shape in the form: (seg_size, 1, 25, 3)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39m# This is to treat each frame as a form of single channel input image\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mreshape(kps, (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseg_size, \u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkp_shape[\u001b[39m0\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkp_shape[\u001b[39m1\u001b[39;49m])), action_class\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/numpy/core/fromnumeric.py:298\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[1;32m    199\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreshape\u001b[39m(a, newshape, order\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    200\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[39m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39m           [5, 6]])\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39mreshape\u001b[39;49m\u001b[39m'\u001b[39;49m, newshape, order\u001b[39m=\u001b[39;49morder)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 3750 into shape (50,1,18,3)"
     ]
    }
   ],
   "source": [
    "cf = load_test_model(params, model_path=params.model_path)\n",
    "df_cm = pd.DataFrame(cf, index=[str(i) for i in range(1, 14)], columns=[str(i) for i in range(1, 14)])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "fefe682e2a409943c6cf3aa8aa94b381342f6a4a6a32468f7a34ff06c072a623"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
